```json
[
  {
    "url": "https://ieeexplore.ieee.org/abstract/document/9770283/",
    "abstract": "",
    "introduction": "",
    "methodology": "",
    "results": "",
    "dataset": ""
  },
  {
    "url": "https://www.sciencedirect.com/science/article/pii/S0167865522000502",
    "abstract": "",
    "introduction": "",
    "methodology": "",
    "results": "",
    "dataset": ""
  },
  {
    "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Kotar_Contrasting_Contrastive_Self-Supervised_Representation_Learning_Pipelines_ICCV_2021_paper.html",
    "abstract": "In the past few years, we have witnessed remarkable breakthroughs in self-supervised representation learning. Despite the success and adoption of representations learned through this paradigm, much is yet to be understood about how different training methods and datasets influence performance on downstream tasks. In this paper, we analyze contrastive approaches as one of the most successful and popular variants of self-supervised representation learning. We perform this analysis from the perspective of the training algorithms, pre-training datasets and end tasks. We examine over 700 training experiments including 30 encoders, 4 pre-training datasets and 20 diverse downstream tasks. Our experiments address various questions regarding the performance of self-supervised models compared to their supervised counterparts, current benchmarks used for evaluation, and the effect of the pre-training data on end task performance. We hope the insights and empirical evidence provided by this work will help future research in learning better visual representations.",
    "introduction": "",
    "methodology": "",
    "results": "",
    "dataset": ""
  },
  {
    "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Wang_Solving_Inefficiency_of_Self-Supervised_Representation_Learning_ICCV_2021_paper.html",
    "abstract": "Self-supervised learning (especially contrastive learning) has attracted great interest due to its huge potential in learning discriminative representations in an unsupervised manner. Despite the acknowledged successes, existing contrastive learning methods suffer from very low learning efficiency, e.g., taking about ten times more training epochs than supervised learning for comparable recognition accuracy. In this paper, we reveal two contradictory phenomena in contrastive learning that we call under-clustering and over-clustering problems, which are major obstacles to learning efficiency. Under-clustering means that the model cannot efficiently learn to discover the dissimilarity between inter-class samples when the negative sample pairs for contrastive learning are insufficient to differentiate all the actual object classes. Over-clustering implies that the model cannot efficiently learn features from excessive negative sample pairs, forcing the model to over-cluster samples of the same actual classes into different clusters. To simultaneously overcome these two problems, we propose a novel self-supervised learning framework using a truncated triplet loss. Precisely, we employ a triplet loss tending to maximize the relative distance between the positive pair and negative pairs to address the under-clustering problem; and we construct the negative pair by selecting a negative sample deputy from all negative samples to avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model. We extensively evaluate our framework in several large-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results demonstrate our model's superiority (e.g., the learning efficiency) over the latest state-of-the-art methods by a clear margin. See Codes at: https://github.com/wanggrun/triplet.",
    "introduction": "",
    "methodology": "",
    "results": "",
    "dataset": ""
  },
  {
    "url": "https://ieeexplore.ieee.org/abstract/document/9522151/",
    "abstract": "",
    "introduction": "",
    "methodology": "",
    "results": "",
    "dataset": ""
  },
  {
    "url": "https://ieeexplore.ieee.org/abstract/document/9462394/",
    "abstract": "",
    "introduction": "",
    "methodology": "",
    "results": "",
    "dataset": ""
  },
  {
    "url": "https://www.sciencedirect.com/science/article/pii/S0010482521009082",
    "abstract": "",
    "introduction": "",
    "methodology": "",
    "results": "",
    "dataset": ""
  },
  {
    "url": "https://link.springer.com/chapter/10.1007/978-3-031-19827-4_28",
    "abstract": "",
    "introduction": "",
    "methodology": "",
    "results": "",
    "dataset": ""
  },
  {
    "url": "https://ieeexplore.ieee.org/abstract/document/10233092/",
    "abstract": "",
    "introduction": "",
    "methodology": "",
    "results": "",
    "dataset": ""
  },
  {
    "url": "http://openaccess.thecvf.com/content/CVPR2024/html/Wang_GroupContrast_Semantic-aware_Self-supervised_Representation_Learning_for_3D_Understanding_CVPR_2024_paper.html",
    "abstract": "Self-supervised 3D representation learning aims to learn effective representations from large-scale unlabeled point clouds. Most existing approaches adopt point discrimination as the pretext task which assigns matched points in two distinct views as positive pairs and unmatched points as negative pairs. However this approach often results in semantically identical points having dissimilar representations leading to a high number of false negatives and introducing a semantic conflict problem. To address this issue we propose GroupContrast a novel approach that combines segment grouping and semantic-aware contrastive learning. Segment grouping partitions points into semantically meaningful regions which enhances semantic coherence and provides semantic guidance for the subsequent contrastive representation learning. Semantic-aware contrastive learning augments the semantic information extracted from segment grouping and helps to alleviate the issue of semantic conflict. We conducted extensive experiments on multiple 3D scene understanding tasks. The results demonstrate that GroupContrast learns semantically meaningful representations and achieves promising transfer learning performance.",
    "introduction": "",
    "methodology": "",
    "results": "",
    "dataset": ""
  }
]
```